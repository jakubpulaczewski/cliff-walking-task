# cliff-walking-task
Re-implemented the resultes presented in Figure 6.4 of the Sutton and Button book comparing SARSA and Q-learning in the cliff-walking task. It is important to note that for this problem, I used 𝛼 = 0.1 and 𝛾 = 1 for both algorithms.

Both Sarsa and Q-learning are temporal difference (TD) learning methods. The benefits of using TD learning is that an agent can learn directly from experience just like with Monte Carlo method; that is, the model of the envrionment is not needed. Furthermore, TD method boostraps which means that it updates estimates based on other learned estimates.

The trade-off exploration and exploitation falls intoo two main classes 1) on-policy (Sarsa) and 2) off-policy (Q-Learning). 

##SARSA


##Q-Learning


# Results
